# # code/configs/nasdaq_transformer.yaml
# # Features include microstructure + adaptive MAs (DEMA/TEMA/HMA/KAMA), TRIX, GARCH.

# model_type: transformer

# data:
#   train_file: src/data/processed/NASDAQ_ITCH/nasdaq_AAPL_20ms_train.parquet
#   val_file:   src/data/processed/NASDAQ_ITCH/nasdaq_AAPL_20ms_val.parquet
#   test_file:  src/data/processed/NASDAQ_ITCH/nasdaq_AAPL_20ms_test.parquet
#   label_col:  y
#   # time_col defaults to 'timestamp' if present in frames

# model:
#   seq_length: 50          # 1.0 s history on a 20 ms grid (50 bars)
#   d_model:     64
#   num_heads:    4
#   num_layers:   2
#   ff_dim:     128
#   dropout_rate: 0.1

# training:
#   epochs:        100
#   batch_size:    512
#   learning_rate: 0.0005
#   patience:      100
#   class_weight:  true
#   output_model_file:  results/models/nasdaq_tf.keras
#   output_scaler_file: results/scalers/nasdaq.json


# code/configs/nasdaq_transformer.yaml
# Features include microstructure + adaptive MAs (DEMA/TEMA/HMA/KAMA), TRIX, GARCH.

# model_type: transformer

# data:
#   train_file: src/data/processed/NASDAQ_ITCH/nasdaq_AAPL_10ms_train.parquet
#   val_file:   src/data/processed/NASDAQ_ITCH/nasdaq_AAPL_10ms_val.parquet
#   test_file:  src/data/processed/NASDAQ_ITCH/nasdaq_AAPL_10ms_test.parquet
#   label_col:  y
#   # time_col defaults to 'timestamp' if present in frames

# model:
#   seq_length: 100         #  1.0 s history on a 10 ms grid (100 bars)
#   d_model:     64
#   num_heads:    4
#   num_layers:   2
#   ff_dim:     128
#   dropout_rate: 0.1

# training:
#   class_weight: true
#   use_focal_loss: false      # ← disable to avoid early collapse
#   focal_gamma: 2.0
#   label_smoothing: 0.02
#   use_reduce_lr: true
#   max_class_weight: 5.0      # ← cap inverse-freq weights
#   learning_rate: 5e-4
#   epochs: 100
#   batch_size: 128
#   patience: 40               # ← train longer so it can escape collapse
#   output_model_file:  results/models/nasdaq_tf.keras
#   output_scaler_file: results/scalers/nasdaq.json


model_type: transformer

data:
  train_file: src/data/processed/NASDAQ_ITCH/nasdaq_AAPL_10ms_train.parquet
  val_file:   src/data/processed/NASDAQ_ITCH/nasdaq_AAPL_10ms_val.parquet
  test_file:  src/data/processed/NASDAQ_ITCH/nasdaq_AAPL_10ms_test.parquet
  label_col:  y
# data:
#   train_file: src/data/processed/NASDAQ_ITCH/nasdaq_AAPL_10ms_train_v2.parquet
#   val_file:   src/data/processed/NASDAQ_ITCH/nasdaq_AAPL_10ms_val_v2.parquet
#   test_file:  src/data/processed/NASDAQ_ITCH/nasdaq_AAPL_10ms_test_v2.parquet
#   label_col: y
#   time_col: timestamp

# model:
#   seq_length: 100
#   d_model:     64
#   num_heads:    4
#   num_layers:   2
#   ff_dim:     128
#   dropout_rate: 0.1






#----------------------------------------------------------

model:
  # core
  seq_length: 100
  d_model: 64
  num_heads: 4
  num_layers: 2
  ff_dim: 128
  dropout_rate: 0.01

  # (optional) conv stem before Transformer (legacy options)
  conv_layers: 1
  conv_filters: 64
  conv_kernel: 5
  conv_stride: 1
  conv_padding: causal
  conv_activation: relu

  # NEW: conv head on top of Transformer
  conv_head_layers: 3            # e.g., 3 residual/dilated blocks
  conv_head_filters: 64
  conv_head_kernel: 5
  conv_head_dilations: [1, 2, 4]   # or omit to auto-use [1,2,4]
  conv_head_separable: true        # depthwise-separable convs
  conv_head_activation: relu       # relu|gelu|swish
  conv_head_dropout: 0.05
  conv_head_padding: causal        # causal or same
  use_se: true
  se_ratio: 0.25
  pool_type: gap_attn              # gap|attn|gap_attn
  head_dropout: 0.05
  head_hidden: 0

# ------------------------------------------------------------







# training:
#   # imbalance controls
#   class_weight: true
#   max_class_weight: 5.0     # ← cap (reduce if still unstable)
#   min_class_weight: 1.0
#   class_weight_power: 1.0

#   # loss controls
#   use_focal_loss: false     # ← disable focal for stability first
#   focal_gamma: 2.0
#   label_smoothing: 0.0

#   # optimization
#   learning_rate: 5e-4
#   use_reduce_lr: true
#   epochs: 120
#   batch_size: 128
#   patience: 40              # ← give it time to escape collapse

#   # checkpointing on validation accuracy
#   monitor: val_accuracy
#   output_model_file:  results/models/nasdaq_tf.keras
#   output_scaler_file: results/scalers/nasdaq.json

# ------------------------------------------------------------

training:
  # imbalance
  class_weight: true
  max_class_weight: 4.0   # cap
  min_class_weight: 1.0   # floor
  class_weight_power: 0.01 # exponent

  # loss
  use_focal_loss: true
  focal_gamma: 2.0
  label_smoothing: 0.02

  # optimization
  learning_rate: 5e-4
  use_reduce_lr: true
  epochs: 10
  batch_size: 128
  patience: 40
  monitor: val_accuracy
  # monitor: 'val_loss'

  output_model_file:  results/models/nasdaq_tf.keras
  output_scaler_file: results/scalers/nasdaq.json


#----------------------------------------------------------



# training:
#   class_weight: true
#   max_class_weight: 3.0
#   min_class_weight: 1.0
#   class_weight_power: 0.5

#   use_focal_loss: false
#   focal_gamma: 2.0
#   label_smoothing: 0.02

#   learning_rate: 5e-4
#   use_reduce_lr: true
#   epochs: 10
#   batch_size: 128
#   patience: 40

#   monitor: val_macro_f1   # ← no balanced accuracy; can switch to val_accuracy if you prefer

#   output_model_file:  results/models/nasdaq_tf.keras
#   output_scaler_file: results/scalers/nasdaq.json

# model:
#   seq_length: 100
#   d_model: 64
#   num_heads: 4
#   num_layers: 2
#   ff_dim: 128
#   dropout_rate: 0.1

#   # optional conv stem
#   conv_layers: 1
#   conv_filters: 64
#   conv_kernel: 5
#   conv_stride: 1
#   conv_padding: causal
#   conv_activation: relu

#   # conv head on top of Transformer
#   conv_head_layers: 3
#   conv_head_filters: 64
#   conv_head_kernel: 5
#   conv_head_dilations: [1,2,4]
#   conv_head_separable: true
#   conv_head_activation: relu
#   conv_head_dropout: 0.05
#   conv_head_padding: causal
#   use_se: true
#   se_ratio: 0.25
#   pool_type: gap_attn
#   head_dropout: 0.05
#   head_hidden: 0

  